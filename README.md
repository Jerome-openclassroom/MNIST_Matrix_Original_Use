# MNIST CNN with TensorFlow + TensorBoard

This project implements a convolutional neural network (CNN) trained on the MNIST dataset using TensorFlow and visualized with TensorBoard. It includes a classic evaluation and a novel interpretation of the confusion matrix as a geometric-statistical object.

**The model and methodology are directly inspired by the pedagogical framework of FIDLE (Formation Ã  lâ€™Intelligence Artificielle pour les DonnÃ©es de Laboratoire et dâ€™ExpÃ©riences) developed by CNRS.**

---

## ğŸ—‚ï¸ Methodology

### ğŸ“¥ Dataset

The MNIST dataset is loaded directly from Hugging Face (`ylecun/mnist`). This dataset was historically curated by **Yann LeCun**, one of the inventors of convolutional neural networks. Originally used for recognizing handwritten digits on U.S. bank checks, it formed the basis for LeNet-5 â€” one of the first CNN architectures, implemented in **C and Fortran** in the late 1990s.

### âš™ï¸ Configuration

- CPU: AMD Ryzen 5  
- RAM: 32 GB  
- OS: Windows 11  
- No GPU acceleration (CPU-only training, 5 epochs under 20 seconds)  
- JupyterLab + TensorBoard

### ğŸ§  Tools

- TensorFlow 2.15  
- Matplotlib (visualizations)  
- scikit-learn (confusion matrix)  
- TensorBoard (histograms, learning curves, training profile)  

---

## ğŸ” Original contribution

### ğŸ“Œ Beyond the confusion matrix

In addition to classic accuracy evaluation (`model.evaluate()`), this project proposes an **original geometric and statistical analysis** of the confusion matrix:

- The **trace** is interpreted as the axis of perfect predictions (i = j)
- The matrix is seen as a **weighted cloud of prediction points**
- We compute:
  - **Mean Absolute Error (MAE)** â†’ average deviation from the diagonal
  - **Mean Squared Error (MSE)** â†’ emphasis on distant/confused predictions
  - **Off-diagonal mass** â†’ total error rate as dispersion
- This view is especially useful when classes are ordered or continuous (e.g., facial similarity, stages of development, ecological gradients)

About the histogram of biases (Tensorflow):  
- At epoch 0, the bias distribution is like **Mont Blanc**: sharp and centered  
- As training proceeds, it evolves into **Puy de DÃ´me**: wider and flatter, reflecting learned diversity and specialization

---

## ğŸ“Š Results

- Final accuracy (test set): **98.36â€¯%**
- Training time (CPU, 5 epochs): **< 20 seconds**
- TensorBoard used to inspect:
  - Learning curves
  - Bias distributions
  - Weight evolution

---

## ğŸ“ Files included

- `Training_MNIST.html` â€“ Exported notebook as HTML  
- `Training_MNIST.md` â€“ Clean Markdown version of the Python code  
- `confusion_matrix.jpg` â€“ Static export of the confusion matrix  
- `histogram_biases.jpg` â€“ Screenshot from TensorBoard  
- `Accuracy_and_loss` â€“ Footage from the Learning curves 
- `Numbers_to_identify_example` â€“ Footage from dataset itself
- `README.md` â€“ This file

---

## âœï¸ Author

**JerÃ´me-X1**  
This repository is part of a broader AI-education and cognitive analysis effort in preparation for the AGI horizon (**mid to late 2026**).
